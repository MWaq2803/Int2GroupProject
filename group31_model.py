# -*- coding: utf-8 -*-
"""group31_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qOrBQZFa2ye0L_TqP5ujpYopiq36HTxc

# Int2 - oxford_flowers102

## Python Library Imports
"""

!pip install -q -U tensorflow_addons

#Import libraries
import tensorflow as tf
import tensorflow_datasets as tfds

import tensorflow_addons as tfa

import matplotlib.pyplot as plt
import numpy as np

from tensorflow.keras import layers

"""## Load and Split Dataset"""

#load the dataset using the tensorflow_datasets load function
ds, info = tfds.load('oxford_flowers102', with_info = True, as_supervised=True)

#Allocate the individual datasets from the combined set
training_set = ds['train']
testing_set = ds['test']
validation_set = ds['validation']
print(type(training_set))

#Check - Print the number of labels
dsfeatures = info.features['label']
label_num = dsfeatures.num_classes
print(dsfeatures)

"""### Scaling"""

#Resizing and rescaling so all images are uniform in size according to constant IMG_SIZE
IMG_SIZE = 255

rescaling = tf.keras.Sequential([
    layers.Resizing(IMG_SIZE,IMG_SIZE),
    layers.Rescaling(1./255)
])

"""## Augmentation

### Augmentation Layers
"""

#Flipping the image - The function defaults to parameter "horizontal_and_vertical"
flipping = tf.keras.Sequential([
    layers.RandomFlip(),
])

#Rotating the image - Randomly rotates the images.
rotating = tf.keras.Sequential([
    layers.RandomRotation(0.2),
])

#Zooming in the image - Randomly zooms in on the images.
zooming = tf.keras.Sequential([
    layers.RandomZoom(0.5)
])

img_contrast = tf.keras.Sequential([
    layers.RandomContrast((0,1.5))    
])

"""### Augment Collator"""

#Preprocessing that collates all all augmentation functionality together
augmentation = tf.keras.Sequential([
    flipping,
    rotating,
    img_contrast
])

def random_brightness(image, label):
  image = tf.image.random_brightness(image, max_delta=0.2)
  return (image,label)
def random_contrast(image, label):
  image = tf.image.random_contrast(image, 0.1, 0.8)
  return (image, label)
def random_flip_horizontal(image, label):
  image = tf.image.random_flip_left_right(image, 5)
  return (image, label)
def random_flip_vertical(image, label):
  image = tf.image.random_flip_up_down(image, 5)
  return (image, label)
def random_hue(image, label):
  image = tf.image.random_hue(image, 0.2)
  return (image, label)
def random_saturation(image,label):
  image = tf.image.random_saturation(image, 5, 10)
  return (image, label)

"""## Model Preperation"""

#Batch size allocation. The number of images that are processed before the model is updated
batch_size = 32

#Prepare for training by batching the sets and shuffling the training set
#Autotune combined with prefetch function allows for increased efficiency while training
#Prefetch allows the model to read the next steps data whilst executing training on the current step
#AUTOTUNE automatically determines the best amount of future steps to prepare for training
AUTOTUNE = tf.data.AUTOTUNE

def preparation(set_type,shuffle=False,augment=False):
  if(shuffle == True):
    set_type = set_type.shuffle(1000)

  #Scales the datasets using rescaling function
  set_type = set_type.map(lambda x, y: (rescaling(x), y), num_parallel_calls = AUTOTUNE)

  if augment:
    set_type = set_type.concatenate(set_type.map(lambda x, y: (augmentation(x, training=True), y),num_parallel_calls=AUTOTUNE))

  set_type = set_type.batch(batch_size)
  
  return set_type.prefetch(buffer_size = AUTOTUNE)

"""
## **Augementation**"""

training_set = preparation(training_set,shuffle=True,augment=True)
random_brightness_set = training_set.map(random_brightness, num_parallel_calls=AUTOTUNE)
random_contrast_set = training_set.map(random_contrast, num_parallel_calls=AUTOTUNE)
random_flip_horizontal_set = training_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_flip_vertical_set = training_set.map(random_flip_vertical, num_parallel_calls=AUTOTUNE)

random_all = training_set.map(random_brightness, num_parallel_calls=AUTOTUNE)
random_all = random_all.map(random_contrast, num_parallel_calls=AUTOTUNE)
random_all = random_all.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_all = random_all.map(random_flip_vertical, num_parallel_calls=AUTOTUNE)

random_contrast_flip_set = training_set.map(random_contrast, num_parallel_calls=AUTOTUNE)
random_contrast_flip_set = random_contrast_flip_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_contrast_flip_set = random_contrast_flip_set.map(random_flip_vertical, num_parallel_calls=AUTOTUNE)

random_brightness_flip_set = training_set.map(random_brightness, num_parallel_calls=AUTOTUNE)
random_brightness_flip_set = random_brightness_flip_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_brightness_flip_set = random_brightness_flip_set.map(random_flip_vertical, num_parallel_calls=AUTOTUNE)

random_flip_contrast_set = training_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_flip_contrast_set = random_flip_contrast_set.map(random_flip_vertical, num_parallel_calls=AUTOTUNE)
random_flip_contrast_set = random_flip_contrast_set.map(random_contrast, num_parallel_calls=AUTOTUNE) 

random_flip_brightness_set = training_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_flip_brightness_set = random_flip_brightness_set.map(random_flip_vertical, num_parallel_calls=AUTOTUNE)
random_flip_brightness_set = random_flip_brightness_set.map(random_brightness, num_parallel_calls=AUTOTUNE)
random_flip_flip_set = training_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)
random_flip_flip_set = random_flip_flip_set.map(random_flip_vertical, num_parallel_calls=AUTOTUNE) 
random_flip_flip_set = random_flip_flip_set.map(random_flip_horizontal, num_parallel_calls=AUTOTUNE)


training_set = training_set.concatenate(random_brightness_set)
training_set = training_set.concatenate(random_contrast_set)
training_set = training_set.concatenate(random_flip_horizontal_set)
training_set = training_set.concatenate(random_flip_vertical_set)
training_set = training_set.concatenate(random_all)
training_set = training_set.concatenate(random_contrast_flip_set)
training_set = training_set.concatenate(random_brightness_flip_set)
training_set = training_set.concatenate(random_flip_contrast_set)
training_set = training_set.concatenate(random_flip_brightness_set)
testing_set = preparation(testing_set)
validation_set = preparation(validation_set)
print(training_set.cardinality().numpy() * batch_size)

"""Setting up the ResNet"""

from keras.regularizers import l2
from keras import backend as K
""""
The residual_module function generates one layer of our ResNet

Parameters:
- data: input to the residual module
- K: number of filters that will be learned by the final CONV layer (the first two CONV layers will learn K/4 filters)
- stride: controls the stride of the convolution (will help us reduce spatial dimensions without using max pooling)
- chanDim: defines the axis which will perform batch normalization
- reduce: will control whether we are reducing spatial dimensions (True) or not (False) as not all residual modules will reduce dimensions of our spatial volume
- reg: applies regularization strength for all CONV layers in the residual module
- bnEps: controls the Ɛ responsible for avoiding “division by zero” errors when normalizing inputs
- bnMom: controls the momentum for the moving average
"""
def residual_module(data, K, stride, chanDim, reduce = False, reg = 0.0001, bnEps = 2e-5, bnMom=0.9):
    #Shortcut branch of the ResNet module
    #Intialized same as the input (data)
    shortcut = data

    #First block, the 1x1 CONVS
    bn1 = layers.BatchNormalization(axis = chanDim, epsilon = bnEps, momentum = bnMom)(data)
    act1 = layers.Activation("relu")(bn1)
    conv1 = layers.Conv2D(int(K * 0.25), (1, 1), use_bias=False, kernel_regularizer = l2(reg))(act1)
    drop1 = layers.Dropout(0.2) (conv1)

    #Second block, the 3x3 CONVs
    bn2 = layers.BatchNormalization(axis = chanDim, epsilon = bnEps, momentum = bnMom)(drop1)
    act2 = layers.Activation("relu")(bn2)
    conv2 = layers.Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = "same", use_bias=False, kernel_regularizer = l2(reg))(act2)
    drop2 = layers.Dropout(0.2) (conv2)

    #Third block, another 1x1 CONVS
    bn3 = layers.BatchNormalization(axis = chanDim, epsilon = bnEps, momentum = bnMom)(drop2)
    act3 = layers.Activation("relu")(bn3)
    conv3 = layers.Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act3)
    drop3 = layers.Dropout(0.2) (conv3)

    #If we are to reduce the spatial size, apply a CONV layer to the shotcut layer
    if reduce:
       shortcut = layers.Conv2D(K, (1, 1), strides = stride, use_bias = False, kernel_regularizer = l2(reg)) (act1)
    
    #Add the final CONV and shortcut together
    output = tf.keras.layers.add([drop3, shortcut])

    return output

"""
The build_ResNet function builds a residual network
Parameters:
- width: width of the dataset
- height: height of the dataset
- depth: depth of the dataset
- classes: number of classes in the dataset
- stages: list of the numbers of residual modules for each filter value
- filters: list of the number of filters used, the first fitler is applied to the only CONV layer not part of the residual module
- reg: applies regularization strength for all CONV layers in the residual module
- bnEps: controls the Ɛ responsible for avoiding “division by zero” errors when normalizing inputs
- bnMom: controls the momentum for the moving average
"""
def build_ResNet(width, height, depth, classes, stages, filters, reg=0.0001, bnEps = 2e-5, bnMom = 0.9):
    inputShape = (height, width, depth)
    chanDim = -1

    #If we are using "channels first", update the input shape and channels dimension
    if K.image_data_format() == "channels_first":
      inputShape = (depth, height, width)
      chanDim = 1

    #Set the input, augment and apply BN
    inputs = layers.Input(shape=inputShape)
    x = layers.BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom)(inputs)
    # x = layers.Activation("relu")(x) chatGPT model

    #Apply CONV then BN then ACT and then POOL to reduce spatial size
    x = layers.Conv2D(filters[0], (5, 5), use_bias=False,padding="same", kernel_regularizer=l2(reg)) (x)
    x = layers.BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom) (x)
    x = layers.Activation("relu")(x)
    x = layers.ZeroPadding2D((1, 1))(x)
    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)
    
    #Loop through the stages
    for i in range(0, len(stages)):
      #Initialize the stride, then apply a residual module
      if i == 0:
         stride = (1,1)
      else:
         stride = (2,2)
      x = residual_module(x, filters[i + 1], stride, chanDim, reduce=True, bnEps=bnEps, bnMom=bnMom)

      #Loop over the number of layers in the stage
      for j in range(0, stages[i]-1):
        #Apply the residual module
        x = residual_module(x, filters[i + 1],(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)
    
    #To avoid dense fully connected layers we apply average pooling to reduce the volume size to 1 x 1 x classes
		#Apply BN then ACT and then POOL
    x = layers.BatchNormalization(axis=chanDim, epsilon=bnEps,momentum=bnMom) (x)
    x = layers.Activation("relu")(x)
    x = layers.AveragePooling2D((8, 8))(x)

    # Add fully connected layer and softmax activation
    x = layers.Flatten() (x)
    x = layers.Dropout(0.2) (x)
    x = layers.Dense(classes, kernel_regularizer=l2(reg))(x)
    x = layers.Activation("softmax")(x)

    #Build the model
    model = tf.keras.models.Model(inputs, x, name="ResNet")
    return model

model2 = build_ResNet(255,255,3,label_num,stages=[1,1,1],filters=[128,64,32,16]) #-- 38.7% on 30 epochs

from tensorflow.keras.optimizers import AdamW

learning_rate = 0.0009
weight_decay = 0.005
beta_1 = 0.8

adamW = AdamW(learning_rate = learning_rate, weight_decay = weight_decay, beta_1 = beta_1)

model2.compile(optimizer = adamW,
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = ['accuracy'])

model2.compile(optimizer = tf.keras.optimizers.AdamW(),
              loss = tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics = ['accuracy'])

"""## Training

Setting up checkpoints to save the model
"""

import os
checkpoint_path = "saves/cp.ckpt"
checkpoint_dir = os.path.dirname(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_freq='epoch', monitor='val_loss', save_best_only=True, save_weights_only= True)
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0.00025,
    patience=7,
    verbose=1,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=100
)

"""### **NOTE**

Using keras_cv layers requires a gpu at runtime so have to run in colab unless specifically downloaded the gpu version of tensorflow which the windows and mac latest versions don't support.

Using just random augments gives accuracy of 0.0098
with AutoContrast gives accuracy of 0.0098

"""

epochs = 30
training = model2.fit(
    training_set,
    validation_data = validation_set,
    epochs = epochs,
    callbacks = [cp_callback],
    verbose = 1
)

model2.load_weights(checkpoint_path)

"""## Model Data"""

model2.summary()

plt.plot(training.history['accuracy'], label='training accuracy')
plt.plot(training.history['val_accuracy'], label='validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
#plt.legend()
plt.grid()
plt.show()

plt.plot(training.history['loss'], label='training loss')
plt.plot(training.history['val_loss'], label='validation loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
#plt.legend()
plt.grid()
plt.show()

loss, accuracy = model2.evaluate(testing_set)
print("Model Accuracy =", accuracy * 100,"% on the test set")

"""## Saving Model"""

#THIS WAS SET UP TO RUN IN A VENV ON MY PC SO PROBABLY WON'T WORK WHEN RAN IN COLAB.
#ALL THIS DOES IS SAVE THE MODEL, READING A MODEL BACK IN NOT YET IMPLEMENTED.

!mkdir -p model_out
model2.save('model_out/model')

from google.colab import drive
drive.mount('/content/drive')